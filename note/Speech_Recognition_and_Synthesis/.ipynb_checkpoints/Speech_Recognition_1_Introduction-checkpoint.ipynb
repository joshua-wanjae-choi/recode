{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 음성인식 : 개요 (Speech_Recognition : Introduction)\n",
    "---\n",
    "__참고사항 :__ \n",
    "이 글은 성원용 교수님의 \"음성인식 및 합성\" 강의와 [Deep Learning for Speech Recognition(Adam Coates, Baidu)](https://www.youtube.com/watch?v=g-sndkf7mCs)를 주로 참고하여 정리하였습니다.  \n",
    "정리글이므로 오류가 있을 수 있으니 github issue 란에 제보해주시면 확인 후 수정하겠습니다.  \n",
    "감사합니다.\n",
    "<br \\>\n",
    "\n",
    "### 1. 음성인식의 목표  \n",
    "![empty](./img/goal_of_ASR.png \"goal_of_ASR\")\n",
    "출처: [Deep Learning for Speech Recognition(Adam Coates, Baidu)](https://www.youtube.com/watch?v=g-sndkf7mCs)  \n",
    "\n",
    "음성인식의 목표는 wave 형태의 음성이 들어오면, 음성인식기를 통해 해당 음성을 문자로 표현하여 보여주는 것이다.  \n",
    "<br \\>  \n",
    "\n",
    "### 2. 음성인식이 어려운 이유\n",
    "1. 단어 수가 너무 많다.\n",
    "2. 사람마다 발음하는게 다르다.\n",
    "3. 소음에도 잘 작동해야 한다.  \n",
    "<br \\>  \n",
    "\n",
    "### 3. 전통적 음성인식 파이프라인\n",
    "![empty](./img/traditional_ASR_pipeline.png \"traditional_ASR_pipeline\")\n",
    "출처: [Deep Learning for Speech Recognition(Adam Coates, Baidu)](https://www.youtube.com/watch?v=g-sndkf7mCs)  \n",
    "\n",
    "Statistical Speech Recognition이라는 방법으로 음성 wave를 주파수(frequency) 기반 표현인 spectrogram으로 변환한 뒤, Acoustic Model과 Language Model을 만들어 해당 음성 wave가 가질 수 있는 문자 중 가장 확률이 높은 문자를 리턴해주는 방법을 말한다.   \n",
    "![empty](./img/traditional_ASR_pipeline_2.png \"traditional_ASR_pipeline_2\")  \n",
    "\n",
    "\n",
    "- __Feature representation__  \n",
    "음성인식을 위해 wave 형태의 음성을 주파수 기반 표현인 spectrogram으로 바꿔준다.  \n",
    "아래 예시에서 \"Rice university\"라는 음성 wave를 spectrogram 형태로 표현하고 있다. \n",
    "![https://i.stack.imgur.com/RJnhA.png](https://i.stack.imgur.com/RJnhA.png \"spectrogram\")  \n",
    "출처 : https://yalantis.com/blog/how-to-process-audio-for-your-android-project/  \n",
    "<br \\>\n",
    "\n",
    "- __Pronunciation Model__  \n",
    "음성은 phone이라는 작은 소리단위로 쪼갤 수 있다.  \n",
    "음성인식에서는 phone들을 모아 놓은 phoneme(또는 phones)을 만들어서 사용한다.  \n",
    "phoneme을 나누는 절대적인 기준은 없고, 여러 기준들이 존재한다.  \n",
    "<br \\>\n",
    "예시로 든 \"Rice university\"는 길이가 너무 기니 \"hello\"를 예로 살펴보자  \n",
    "\"hello\"는 \\[HH AH L OW\\]라는 phoneme으로 나타낼 수 있다.  \n",
    "이렇게 single phone을 사용하는 방식을 Monophone이라고 한다.\n",
    "![empty](./img/hello.png \"hello\")  \n",
    "phone보다 더 세세하게 구분할 수도 있다.  \n",
    "Triphone은 기존 single phone을 3등분해서 사용하는 방식을 말한다.\n",
    "![empty](./img/hello_triphone.png \"hello_triphone\")\n",
    "참고 : [What is the different between a monophone and a triphone? ](http://www.voxforge.org/home/docs/faq/faq/what-is-the-different-between-a-monophone-and-a-triphone)  \n",
    "<br \\>\n",
    "Triphone 보다 더 세세하게 나누는 방법으로는 Context dependent phone이 있다.  \n",
    "같은 \"hello\" 라도 앞뒤에 오는 단어에 영향을 받아 그 소리가 조금씩 다를 것이다.  \n",
    "예를 들어 \"hello, frank\"와 \"hello, yumi\"에 등장하는 각각의 \"hello\"들의 wave는 이후 등장하는 단어에 의해 뒷부분이 서로 다르게 된다.  \n",
    "이러한 경우까지 고려해서 phone을 구분하는 방식을 Context dependent phone 이라고 한다.  \n",
    "<br \\>\n",
    "phone을 세세하게 구분하는 경우, 훨씬 다양한 소리를 표현하는  Spectrogram을 얻을 수 있다는 장점이 있지만,  \n",
    "음성데이터에 태깅해야하는 phone 수가 증가하여 training data를 만들기 어렵고, model train 및 inference 시 고려해야할 variants가 증가된다는 단점이 있다.  \n",
    "현대에는 사람이 지정한 phone을 이용하는 것이 아니라, 음성과 문자를 자동으로 매칭해주는 CTC 알고리즘을 이용하고 있다.  \n",
    "<br \\>  \n",
    "\n",
    "- __Acoustic Model (P(O|W))__  \n",
    "Acoustic Model은 해당 음성 wave 주어졌을 때 말그대로 어떤 문자인지 예측하는 모델이다.  \n",
    "같은 단어를 말해도 사람마다 발음하는 것이 다르며, 또 같은 사람이라고 해도 상황에 따라 발음하는 것이 다르다.  \n",
    "이를 잘 학습시키기 위해 같은 단어를 여러 사람이 말한 음성데이터를 가지고 가장 잘 표현할 수 있는 분포를 만드는 데,  \n",
    "전통적으로는 Gaussian Mixture Model(GMM)을 많이 사용하고, 현대에는 neural net을 사용되는 방법이 고안되고 있다.  \n",
    "<br \\>  \n",
    "\n",
    "- __Language Model (P(W))__  \n",
    "Language Model은 어떤 단어가 나올 확률을 나타내는 모델로 Acoustic Model의 결과물을 보정한다.    \n",
    "Acoustic Model이 예측한 문자는 소리를 그대로 문자로 예측한 것이기 때문에 오류가 많을 수 밖에 없다.  \n",
    "왜냐하면 비슷한 소리가 나는 문자도 많고, 때때로 학습 시 사용한 음성데이터 발음이 특이한 경우도 있기 때문이다.  \n",
    "<br \\>\n",
    "예를 들어 학습 데이터를 만드는데 어떤 사람이 \"나는 학교에 갑니다\"를 \"나는 학고에 갑니다\"라고 발음한 경우를 생각해보자.  \n",
    "일반적인 사람이라면 \"학고\"는 \"학교\"를 잘못 발음이라는 사실을 알아차릴 것이다.  \n",
    "하지만 Acoustic Model이 하는 일은 발음을 있는 그대로 문자로 예측하는 것이기 때문에 \"학고\"가 옳다고 예측하게 된다.  \n",
    "<br \\>\n",
    "이를 보정하기 위해 Language Model이 사용된다.  \n",
    "Language Model에 의하면, \"나는 ㅁㅁ에 갑니다\"라는 문장에서 \"학고\"가 등장할 확률보다 \"학교\"가 등장할 확률이 높을 것이다.  \n",
    "\"학고\"와 \"학교\"는 발음이 비슷하기 때문에 Acoustic Model의 확률값의 변화는 크지 않을 것이고,  \n",
    "Language Model에 의해 구해진 확률값은 \"학교\"가 \"확고\"보다 훨씬 크기 때문에, 결과적으로 \"나는 학교에 갑니다\"를 예측하게 된다.  \n",
    "<br \\>\n",
    "\n",
    "### 4. ASR의 구현 방법\n",
    "크게 3가지로 나눌 수 있다.  \n",
    "1. 음성 wave를 MFCC로 변환             --> Acoustic model : GMM, Language model : HMM  \n",
    "2. 음성 wave를 Mel scaled filter bank --> Acoustic model : DNN, Language model : HMM  \n",
    "3. 음성 wave를 Mel scaled filter bank --> RNN(CTC) + 간단한 Decoder  \n",
    "\n",
    "아직까지는 많은 기업이 2번을 쓰고 있으나, 데이터가 많아지면 많아질수록 3번이 유리할 것으로 보인다.\n",
    "![empty](./img/scaled_model.png \"scaled_model\")\n",
    "출처: [Deep Learning for Speech Recognition(Adam Coates, Baidu)](https://www.youtube.com/watch?v=g-sndkf7mCs)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tacotron",
   "language": "python",
   "name": "tacotron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
